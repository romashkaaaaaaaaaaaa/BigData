{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5nVPaRj2Agn"
      },
      "source": [
        "# Робота з Spark SQL\n",
        "## Складна аналітика з Spark SQL\n",
        "\n",
        "> Можна виконати або за допомогою чистого `DataFrame` API, `Pandas on Spark` API, або за допомогою чистих `SQL` запитів. Вибір за вами.\n",
        "\n",
        "Ви розробник у компанії **BikeServe**, яка займається орендою велосипедів/скутерів. У вас є певні місця (\"станції\"), де зберігаються ваші велосипеди. Якщо на станції немає вільних місць для велосипедів (хтось уже зарезервував або взяв велосипед) протягом певного періоду часу (`timeslot`), це означає, що бізнес йде чудово. Однак вам потрібно покращити обслуговування клієнтів, пропонуючи користувачам велосипеди, коли та в тому місці, де це для них найважливіше.\n",
        "\n",
        "Ваше завдання — знайти найбільш важливі (\"критичні\") пари станції та періоду часу `(stationId, timeslot)`, щоб ваш бізнес знав, куди і коли доставити більше велосипедів.\n",
        "\n",
        "Ваш результат має бути відсортований за цією *критичністю* у порядку спадання.\n",
        "\n",
        "Набір даних містить:\n",
        "* `register.csv` містить інформацію з вашої IoT системи моніторингу про кількість використаних і вільних слотів на ваших станціях оренди велосипедів. Кожен рядок відповідає одному запису про ситуацію на одній станції в певний момент часу.\n",
        "\n",
        "    Кожен рядок має такий формат:\n",
        "\n",
        "    ```bash\n",
        "    stationId\\ttimestamp\\tusedslots\\tfreeslots\n",
        "    ```\n",
        "    \n",
        "    де `timestamp` має формат `datetime`.\n",
        "\n",
        "    > Перший рядок файлу містить заголовок.\n",
        "    > Деякі дані в наборі даних пошкоджено через тимчасові збої мережі та/або вашої системи моніторингу. Це означає, що деякі рядки характеризуються \"використаними слотами (used slots) = 0\" і \"вільними слотами (free slots) = 0\". **Ці рядки необхідно відфільтрувати** перед виконанням будь-яких операцій.\n",
        "\n",
        "* `input/stations.csv` містить опис станцій.\n",
        "\n",
        "    Кожен рядок має такий формат:\n",
        "\n",
        "    ```bash\n",
        "    stationId\\tlongitude\\ttitude\\tname\n",
        "    ```\n",
        "    > Перший рядок файлу містить заголовок.\n",
        "\n",
        "### Опис завдання\n",
        "\n",
        "Кожна пара \"день тижня – година\" є \"часовим інтервалом\" (`timeslot`) і пов’язана з усіма показаннями моніторингу, пов’язаними з цією парою, незалежно від дати. Наприклад, часовий інтервал `Wednesday - 17` відповідає всім показанням, зробленим у середу з `17:00:00` до `17:59:59`.\n",
        "\n",
        "Станція $S_i$ знаходиться в критичному стані, якщо кількість вільних слотів дорівнює `0` (всі велосипеди на станції заброньовані).\n",
        "\n",
        "*Критичність* станції $S_i$ у часовому інтервалі $T_j$ визначається як:\n",
        "\n",
        "$$\n",
        "\\frac{\\text{кількість записів із числом вільних слотів, яке дорівнює нулю, для пари}_{\\left(S_i,T_j\\right)}}{\\text{загальна кількість записів для пари}_{\\left(S_i,T_j\\right)}}\n",
        "$$\n",
        "\n",
        "необхідно:\n",
        "* Обчислити значення *критичності* для кожної пари $(S_i, T_j)$.\n",
        "* Вибирати лише пари, у яких значення *критичності* перевищує \"мінімальний поріг критичності\".\n",
        "    * `Мінімальний поріг критичності` має бути параметром конфігурації програми.\n",
        "* Зберегти у вихідній папці вибрані записи, використовуючи файли `csv` (із заголовком). Зберегти лише такі атрибути:\n",
        "    * ідентифікатор станції\n",
        "    * день тижня\n",
        "    * година\n",
        "    * критичність\n",
        "    * довгота станції\n",
        "    * широта станції\n",
        "* Зберегти результати за зменшення критичності. Якщо є два або більше записів, що характеризуються однаковим значенням критичності, то додатково відсортувати по ідентифікатору станції (у порядку зростання). Якщо і станція та сама, то сортувати за днем тижня (за зростанням) і, нарешті, за годиною (за зростанням).\n",
        "\n",
        "### Поради та підказки\n",
        "\n",
        "Мова SQL, доступна в Spark SQL, має низку попередньо визначених функцій, одна з яких, `hour(timestamp)`, може використовуватися в запитах SQL або в перетворенні `selectExpr`, щоб вибрати `hour` з заданого позначка часу. Ще одна цікава функція, `date_format(timestamp,format)`, може бути використана для отримання іншої корисної інформації зі стовпця timestamp. Наприклад, у форматі `EE` можна отримати день тижня.\n",
        "\n",
        "```python\n",
        "new_df= df.selectExpr(\"date_format(timestamp,'EE') as weekday hour(timestamp) as hour\")\n",
        "```\n",
        "\n",
        "Щоб вказати, що роздільником вхідних файлів CSV є спеціальний символ `tab`, установіть параметр роздільника на `\\\\t`, викликавши `.option(\"delimiter\", \"\\\\t\")` під час читання вхідних даних."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Sq0tvc2Agq"
      },
      "source": [
        "## Конфігурація\n",
        "\n",
        "- `number_cores`: Кількість ядер, виділених під Spark\n",
        "- `memory_gb`: Обʼєм оперативної памʼяті, виділеної під Spark (в Гб)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDlpl6ml2Agr"
      },
      "outputs": [],
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nKDVj3N2Agt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName('Spark Bikes')\n",
        "    .master(f\"local[{number_cores}]\")\n",
        "    .config(\"spark.driver.memory\", f\"{memory_gb}g\")\n",
        "    .getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwI9wNO_2Agt"
      },
      "source": [
        "## Рішення\n",
        "Прочитайте вміст вхідного файлу `register.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* station: integer (nullable = true)\n",
        "* timestamp: timestamp (nullable = true)\n",
        "* used_slots: integer (nullable = true)\n",
        "* free_slots: integer (nullable = true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iFKt-NZ2Agt"
      },
      "outputs": [],
      "source": [
        "register_df = spark.read.csv(\"register.csv\", header=True, inferSchema=True, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgkyZ8s-2Agt"
      },
      "source": [
        "Видаліть рядки де одночасно `free_slots = 0` та `used_slots = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXEM_Jxh2Agu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "filtered_register_df = register_df.filter((col(\"free_slots\") != 0) | (col(\"used_slots\") != 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN1j8xqT2Agu"
      },
      "source": [
        "Нам потрібен логічний маркер, щоб побачити, заповнена станція чи ні. Це можна зробити за допомогою UDF під назвою `full(free_slots: int)`, яка повертає\n",
        "* 1, якщо `free_slots` дорівнює 0\n",
        "* 0, якщо `free_slots` більше 0\n",
        "\n",
        "> Якщо ви використовуєте Pandas on Spark API, то треба самостійно застосувати цю функцію (або переписати її)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa3DREjr2Agu"
      },
      "outputs": [],
      "source": [
        "def full_status(free_slots):\n",
        "    if free_slots == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "full_status_udf = udf(full_status, IntegerType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fx1SUC2Agu"
      },
      "source": [
        "Створіть DataFrame з такою схемою:\n",
        "* station: integer (nullable = true)\n",
        "* dayofweek: string (nullable = true)\n",
        "* hour: integer (nullable = true)\n",
        "* fullstatus: integer (nullable = true) - 1 = full, 0 = non-full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4Pwg-NM2Agv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "stations_df = filtered_register_df.withColumn(\"fullstatus\", full_status_udf(col(\"free_slots\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIPa1KQ2Agv"
      },
      "source": [
        "Визначте одну групу для кожної комбінації `(station, dayofweek, hour)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9kQ_YOP2Agv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import dayofweek, hour\n",
        "\n",
        "grouped_df = stations_df.withColumn(\"dayofweek\", dayofweek(col(\"timestamp\"))) \\\n",
        "                        .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
        "\n",
        "grouped_df = grouped_df.groupBy(\"station\", \"dayofweek\", \"hour\").agg({\"fullstatus\": \"avg\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS4oK3_n2Agv"
      },
      "source": [
        "Обчисліть \"критичність\" для кожної групи `(station, dayofweek, hour)`, тобто для кожної пари `(station, timeslot)`.\n",
        "\n",
        "Критичність дорівнює середньому `fullStatus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE6Nm-wz2Agw"
      },
      "outputs": [],
      "source": [
        "criticality_df = grouped_df.withColumnRenamed(\"avg(fullstatus)\", \"criticality\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng5-RMD92Agw"
      },
      "source": [
        "Виберіть лише рядки з `criticality > threshold`\n",
        "\n",
        "> `threshold` є деякою бізнес-вимогою, тому візьміть випадкове число від `0.1` до `0.5`, яке вам подобається :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBAKswhs2Agw"
      },
      "outputs": [],
      "source": [
        "threshold = 0.3\n",
        "\n",
        "filtered_criticality_df = criticality_df.filter(col(\"criticality\") > threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd4I_urK2Agx"
      },
      "source": [
        "Прочитайте вміст вхідного файлу `stations.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* id: integer (nullable = true)\n",
        "* longitude: double (nullable = true)\n",
        "* latitude: double (nullable = true)\n",
        "* name: string (nullable = true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIw9xHMp2Agx"
      },
      "outputs": [],
      "source": [
        "stations_info_df = spark.read.csv(\"stations.csv\", header=True, inferSchema=True, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp_4rk6e2Agx"
      },
      "source": [
        "Об’єднайте (`JOIN`) вибрані критичні часові інтервали з таблицею станцій, щоб отримати координати станцій"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7hvl8Sg2Agx"
      },
      "outputs": [],
      "source": [
        "result_df = filtered_criticality_df.join(stations_info_df, filtered_criticality_df[\"station\"] == stations_info_df[\"id\"]).select(\"station\", \"dayofweek\", \"hour\", \"criticality\", \"longitude\", \"latitude\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcI-DKif2Agy"
      },
      "source": [
        "Відсортуйте вміст DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du8us04v2Agy"
      },
      "outputs": [],
      "source": [
        "sorted_result_df = result_df.orderBy(col(\"criticality\").desc(), col(\"station\"), col(\"dayofweek\"), col(\"hour\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTJcMbkw2Agy"
      },
      "source": [
        "Write to file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_J19MPK2Agy"
      },
      "outputs": [],
      "source": [
        "sorted_result_df.write.csv(\"output/critical_pairs_sorted.csv\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8iPHQ9W2Agy"
      },
      "source": [
        "## Зупинка Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj5VdVlB2Agy"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}